# -*- coding: utf-8 -*-
"""기계학습기초_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bGMrVl4Vct1LuB1PXrxWXbBKWmHa2QxX
"""

# Commented out IPython magic to ensure Python compatibility.
# 필요한 라이브러리 로드
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive')

# data import
data = pd.read_csv("/content/drive/MyDrive/data.csv")

# data 확인
print(data)

# 결측치 확인
data.isna().sum()

######## drop 기준 ########
# x, y : LON, LAT과 겹치는 값
# CMEDV, MEDV : 예측값(target)
# ID, TOWN : ID는 순서, TOWN은 0으로 동일한 값으로 학습에 무의미함
data = data.drop(['x', 'y', 'MEDV', 'ID', 'TOWN'], axis = 1)

data.info()

#위도와 경도를 기준으로 target 분포 확인
data.plot(kind='scatter',x="LON", y="LAT", alpha=0.6,
          s = data['CMEDV']*3, label="CMEDV",figsize=(10,7),
          c="CMEDV", cmap=plt.get_cmap("hsv"),colorbar=True, sharex=False)
plt.legend()
plt.show()

# 위치에 따른 plotting 결과를 확인했을 때 위치로 regression하는 것은 힘들어 보임 >> 위치데이터 삭제
data = data.drop(['LON', 'LAT'], axis = 1)

fig, ax = plt.subplots( figsize=(14,14) )

corr_data = data.corr()

# 삼각형 마스크를 만든다(위 쪽 삼각형에 True, 아래 삼각형에 False)
mask = np.zeros_like(corr_data, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True

# 히트맵을 그린다
sns.heatmap(corr_data, 
            cmap = 'RdYlBu_r', 
            annot = True,   # 실제 값을 표시한다
            mask=mask,      # 표시하지 않을 마스크 부분을 지정한다
            linewidths=.5,  # 경계면 실선으로 구분하기
            cbar_kws={"shrink": .5},# 컬러바 크기 절반으로 줄이기
            vmin = -1,vmax = 1   # 컬러바 범위 -1 ~ 1
           )  
plt.show()

# correlation 값들을 내림차순으로 출력한다
corr_data = data.corr().loc[:,'CMEDV'].abs().sort_values(ascending = False)
plot_data =[]
print(corr_data)

# correlation 값이 큰 순서로 6개 feature(target 포함)를 저장한다.
for i in range(6):
  plot_data.append(corr_data.index[i])

# 저장된 feature 확인
print(plot_data)

# 새로은 dataframe에 확인할 feature 저장
data = data.loc[:, plot_data]

# CMEDV를 target으로 저장한 뒤 data에서 삭제
target = data.loc[:,'CMEDV']
data = data.drop(['CMEDV'], axis = 1)

# data에 있는 5개의 feature를 CMEDV와 함께 plotting 후 추세선 계산
plt.figure(figsize=(20,10))
plt.subplots_adjust(left=0.125, bottom=0.1,  right=0.9, top=0.9, wspace=0.4, hspace=0.35)

for count in range(0, len(data.columns)):
  plt.subplot(2, 3, count+1)
  plt.plot(data.iloc[:,count], target, 'ro')
  plt.xlabel(data.columns[count], fontsize = 12)
  plt.ylabel('CMEDV', fontsize = 12)
  sns.regplot(x=data.iloc[:,count], y=target) # 추세선 plotting
  plt.title(data.columns[count], fontsize = 20)

plt.show()

# linear regression에 사용할 2개의 feature 저장
regression_feature = data.loc[:,['LSTAT', 'RM']]

# training set과 test set을 나눔. 비율은 4:1
from sklearn.model_selection import train_test_split

train_feature, test_feature, train_target, test_target = train_test_split(
    regression_feature, target, test_size = 0.2, random_state = 1)

from sklearn.linear_model import LinearRegression # Linear Regression 모델 import
reg_model=LinearRegression()
reg_model.fit(train_feature, train_target)

print("회귀계수(기울기):", np.round(reg_model.coef_, 1))
print("상수항(절편):", np.round(reg_model.intercept_, 1))

train_predict=reg_model.predict(train_feature)
test_predict=reg_model.predict(test_feature)

print("예측: ", test_predict[:5])
print("정답: ", list(test_target[:5]))

from sklearn.metrics import r2_score # R2 score 계산 함수 import

train_r2_score = r2_score(train_target, train_predict)
test_r2_score = r2_score(test_target, test_predict)

print("train R2 score : {:.4f}".format(train_r2_score))
print("test R2 score : {:.4f}".format(test_r2_score))

from sklearn.metrics import mean_squared_error # MSE 계산 함수 import

train_rmse=mean_squared_error(train_target, train_predict)
train_rmse=np.sqrt(train_mse) # MSE 계산후 루트를 씌워주면 RMSE가 나온다.
print("train RMSE: {:.4f}".format(train_rmse))

test_mse=mean_squared_error(test_target, test_predict)
test_rmse=np.sqrt(test_mse)
print("test RMSE: {:.4f}".format(test_rmse))

from sklearn.metrics import mean_squared_log_error # MSLE 계산함수 import

# train_predict값에 음수값이 존재해 error가 발생함.
# 과제의 목적은 평가지표를 보여주고 설명하는 것이기에 0으로 바꿔주고 진행하였음.
# 0으로 바꾸어 주었기 때문에 오차값에 있어서 최대라고 생각함.
for i in range(len(train_predict)):
  if train_predict[i] <0:
    train_predict[i] = 0

train_msle=mean_squared_log_error(train_target, train_predict)
test_msle=mean_squared_log_error(test_target, test_predict)
# MSLE 계산 후 루트를 씌우면 RMSLE가 나온다.
train_rmsle = np.sqrt(train_msle)
test_rmsle = np.sqrt(test_msle)

print("train RMLSE: {:.4f}".format(train_rmsle))
print("test RMLSE: {:.4f}".format(test_rmsle))

plt.figure(figsize = (16,9))
plt.rc('font', size=15)        # 기본 폰트 크기
plt.rc('axes', labelsize=6)   # x,y축 label 폰트 크기
plt.rc('xtick', labelsize=10)  # x축 눈금 폰트 크기 
plt.rc('ytick', labelsize=10)  # y축 눈금 폰트 크기
plt.rc('legend', fontsize=12)  # 범례 폰트 크기
plt.rc('figure', titlesize=6)

plt.subplots_adjust(left=0.125, bottom=0.1,  right=0.9, top=0.9, wspace=0.2, hspace=0.35)


plt.subplot(2, 2, 1) # trainig의 CMEDV-LSTAT plotting
plt.title('train LSTAT by target')
plt.ylabel('CMEDV', fontsize = 10)
plt.xlabel('LSTAT', fontsize = 10)
plt.scatter(train_feature.loc[:,'LSTAT'], train_target, label="true value")
plt.scatter(train_feature.loc[:,'LSTAT'], train_predict, c='r', label="predicted value")
plt.legend(loc='best') 

plt.subplot(2, 2, 2) # trainig의 CMEDV-RM plotting
plt.title('train RN by target')
plt.ylabel('CMEDV', fontsize = 10)
plt.xlabel('RM', fontsize = 10)
plt.scatter(train_feature.loc[:,'RM'], train_target, label="true value")
plt.scatter(train_feature.loc[:,'RM'], train_predict, c='r', label="predicted value")
plt.legend(loc='best')  

plt.subplot(2, 2, 3) # test의 CMEDV-LSTAT plotting
plt.title('test LSTAT by target')
plt.ylabel('CMEDV', fontsize = 10)
plt.xlabel('LSTAT', fontsize = 10)
plt.scatter(test_feature.loc[:,'LSTAT'], test_target, label="true value")
plt.scatter(test_feature.loc[:,'LSTAT'], test_predict, c='r', label="predicted value")
plt.legend(loc='best') 

plt.subplot(2, 2, 4) # test CMEDV-RM plotting
plt.title('test RN by target')
plt.ylabel('CMEDV', fontsize = 10)
plt.xlabel('RM', fontsize = 10)
plt.scatter(test_feature.loc[:,'RM'], test_target, label="true value")
plt.scatter(test_feature.loc[:,'RM'], test_predict, c='r', label="predicted value")
plt.legend(loc='best')  

plt.show()

# 이해를 위해 x, y, z에 각각 LSTAT, RM, CMEDV 저장
x = test_feature.iloc[:, 0]
y = test_feature.iloc[:, 1]
z = test_target

x_pred = np.linspace(x.min(), x.max(), 10) # x의 최대, 최소를 기준으로 공간 생성
y_pred = np.linspace(y.min(), y.max(), 10) # 위와 동일
xx_pred, yy_pred = np.meshgrid(x_pred, y_pred) # x, y를 이용해 regression된 좌표 계산
model_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T # 앞서 계산한 좌표로 plane 좌표 저장

r2 = reg_model.score(train_feature, train_target) # linear regression

plt.style.use('default')

fig = plt.figure(figsize=(12, 4))

ax1 = fig.add_subplot(131, projection='3d')
ax2 = fig.add_subplot(132, projection='3d')
ax3 = fig.add_subplot(133, projection='3d')
 
axes = [ax1, ax2, ax3]

predicted = reg_model.predict(model_viz)

for ax in axes:
    ax.plot(x, y, z, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5) #검은색 마커들
    ax.scatter(xx_pred.flatten(), yy_pred.flatten(), predicted, s=20, edgecolor='#70b3f0') #파란색 마커들
    ax.set_xlabel('LSTAT', fontsize=12) #해당 축을 설명하는 라벨
    ax.set_ylabel('RM', fontsize=12)
    ax.set_zlabel('CMEDV', fontsize=12)
    ax.locator_params(nbins=8, axis='x') #해당 축의 구간 개수 (쪼개진 구간도 포함)
    ax.locator_params(nbins=8, axis='y')
    ax.locator_params(nbins=8, axis='z')
# 각도 조절
ax1.view_init(elev=0, azim=10) 
ax2.view_init(elev=30, azim=30)
ax3.view_init(elev=60, azim=30)
 
fig.tight_layout()

# 오차값 저장
dif = train_predict - train_target

# 오차값의 평균과 분산을 계산하고, 분포를 plotting
print("dif mean = {:4f} \ndif variation = {:4f}".format(dif.mean(), dif.var()))
plt.scatter(range(len(dif)), dif)

# 모델 저장
import joblib
joblib.dump(reg_model, 'mymodel.pkl')

